{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_XDxrKjjSxP9"
      },
      "source": [
        "# Attention Transfer\n",
        "\n",
        "This notebook demonstrates the implementation of this paper [Paying More Attention to Attention](https://arxiv.org/abs/1612.03928)\n",
        "\n",
        "## Steps to transfer from a teacher to student model\n",
        "\n",
        "- Load dataset and create dataloaders\n",
        "- Create teacher and student models and load pretrained weights of teacher model\n",
        "- Load the configuration YAML file\n",
        "- Create `AttentionTransfer` object and pass the dataloaders, teacher model, student model and configuration\n",
        "- Transfer the knowledge to student model by using `compress_model` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CP9FUD9yVTn3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/py117/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../../../\")\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import yaml\n",
        "\n",
        "from trailmet.algorithms.distill.attention_transfer import AttentionTransfer\n",
        "from trailmet.datasets.classification import DatasetFactory\n",
        "from trailmet.models import ModelsFactory\n",
        "from trailmet.utils import accuracy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pC124keNVTn6"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_mHEwwycVTn8"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z786Cf7ASxQG"
      },
      "source": [
        "### Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0EwK_RVoVTn8"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose(\n",
        "                [\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Pad(4, padding_mode='reflect'),\n",
        "                    transforms.RandomHorizontalFlip(p=0.5),\n",
        "                    transforms.RandomCrop(32),\n",
        "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "                ]\n",
        "            )\n",
        "\n",
        "transform_test = transforms.Compose(\n",
        "                [\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "                ]\n",
        "            )\n",
        "\n",
        "transforms1 = {\n",
        "    'train': transform_train, \n",
        "    'val': transform_test, \n",
        "    'test': transform_test}\n",
        "\n",
        "target_transforms = {\n",
        "    'train': None, \n",
        "    'val': None, \n",
        "    'test': None}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6OL6wyjSxQJ"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHFlUHnSgRTD",
        "outputId": "489a6485-5c60-443f-e676-5b626014d365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "cifar_dataset = DatasetFactory.create_dataset(name = 'CIFAR10', \n",
        "                                        root = \"./data\",\n",
        "                                        split_types = ['train', 'val', 'test'],\n",
        "                                        val_fraction = 0.1,\n",
        "                                        transform = transforms1,\n",
        "                                        target_transform = target_transforms,\n",
        "                                        random_seed=42\n",
        "                                        )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wlwvRRCxSxQN"
      },
      "source": [
        "### Define data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PRVfJ1rig3YY"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "        cifar_dataset['train'], batch_size=128, \n",
        "        sampler=cifar_dataset['train_sampler'],\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "        cifar_dataset['val'], batch_size=128, \n",
        "        sampler=cifar_dataset['val_sampler'],\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        cifar_dataset['test'], batch_size=128, \n",
        "        sampler=cifar_dataset['test_sampler'],\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "dataloaders = {'train':train_loader, 'val':val_loader, 'test':test_loader}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load configurations for training the student model\n",
        "\n",
        "The configuration should contain distillation arguments including the training parameters such as total epochs, learning rates, milestones, etc. as well as the names of layers involved in Attention Transfer\n",
        "\n",
        "__Note:__ Running on 5 epochs for demonstration purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'DISTILL_ARGS': {'BETA': 500,\n",
              "  'EPOCHS': 5,\n",
              "  'LR': 0.1,\n",
              "  'WEIGHT_DECAY': 0.0005,\n",
              "  'MILESTONES': [82, 123],\n",
              "  'GAMMA': 0.1,\n",
              "  'TEACHER_LAYER_NAMES': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
              "  'STUDENT_LAYER_NAMES': ['layer1', 'layer2', 'layer3', 'layer4'],\n",
              "  'VERBOSE': True},\n",
              " 'log_dir': 'at_resnet50-resnet18',\n",
              " 'wandb': True,\n",
              " 'insize': 32,\n",
              " 'DEVICE': 'cuda:0'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"./resnet50-resnet18.yaml\", 'r') as stream:\n",
        "    data_loaded = yaml.safe_load(stream)\n",
        "data_loaded"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "frJVv5qoVTn_"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zQOrdm6dSxQR"
      },
      "source": [
        "### Create the teacher and student models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XifOZKPAVToB",
        "outputId": "32a39b63-a43a-427b-b43a-64ee37927638"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (activ): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activ): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activ): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activ): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activ): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activ): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activ): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activ): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activ): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "teacher_model = ModelsFactory.create_model(name='resnet50', num_classes=100, version=\"original\", pretrained=False, **data_loaded)\n",
        "student_model = ModelsFactory.create_model(name='resnet18', num_classes=100, version=\"original\", pretrained=False, **data_loaded)\n",
        "\n",
        "teacher_model.to(device)\n",
        "student_model.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "awBggHtxSxQU"
      },
      "source": [
        "### Training student model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1f1tmxodVToB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manimesh-007\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/workspace/animesh_trailmet/experiments/distillation/Attention Transfer/wandb/run-20230625_204759-ngmbcp3u</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/animesh-007/Trailmet%20Attention_Transfer/runs/ngmbcp3u' target=\"_blank\">CIFAR10_5_0.1_Jun-25_20:47:57</a></strong> to <a href='https://wandb.ai/animesh-007/Trailmet%20Attention_Transfer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/animesh-007/Trailmet%20Attention_Transfer' target=\"_blank\">https://wandb.ai/animesh-007/Trailmet%20Attention_Transfer</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/animesh-007/Trailmet%20Attention_Transfer/runs/ngmbcp3u' target=\"_blank\">https://wandb.ai/animesh-007/Trailmet%20Attention_Transfer/runs/ngmbcp3u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "distillation_box = AttentionTransfer(teacher_model, student_model, dataloaders, **data_loaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQWD6lfzVToC",
        "outputId": "d122e977-513f-425f-d280-1b27c93b91b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=====> TRAINING STUDENT NETWORK <=====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training student network Epoch [0] (352 / 352 Steps) (batch time=0.05071s) (data time=0.00202s) (loss=2.29474): 100%|| 352/352 [00:19<00:00, 17.66it/s]\n",
            "Validating student network Epoch [0] (40 / 40 Steps) (batch time=0.01823s) (loss=1.98314) (top1=25.00000) (top5=87.50000): 100%|| 40/40 [00:01<00:00, 33.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * acc@1 19.440 acc@5 82.340\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training student network Epoch [1] (352 / 352 Steps) (batch time=0.04935s) (data time=0.00363s) (loss=1.97412): 100%|| 352/352 [00:18<00:00, 18.70it/s]\n",
            "Validating student network Epoch [1] (40 / 40 Steps) (batch time=0.01691s) (loss=1.74551) (top1=37.50000) (top5=100.00000): 100%|| 40/40 [00:01<00:00, 33.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * acc@1 30.880 acc@5 86.520\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training student network Epoch [2] (352 / 352 Steps) (batch time=0.04837s) (data time=0.00243s) (loss=1.65647): 100%|| 352/352 [00:18<00:00, 19.07it/s]\n",
            "Validating student network Epoch [2] (40 / 40 Steps) (batch time=0.01680s) (loss=2.25188) (top1=12.50000) (top5=75.00000): 100%|| 40/40 [00:01<00:00, 35.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * acc@1 36.300 acc@5 85.800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training student network Epoch [3] (352 / 352 Steps) (batch time=0.04921s) (data time=0.00337s) (loss=1.56491): 100%|| 352/352 [00:19<00:00, 18.46it/s]\n",
            "Validating student network Epoch [3] (40 / 40 Steps) (batch time=0.01689s) (loss=1.94252) (top1=50.00000) (top5=87.50000): 100%|| 40/40 [00:01<00:00, 35.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * acc@1 43.560 acc@5 92.000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training student network Epoch [4] (352 / 352 Steps) (batch time=0.04672s) (data time=0.00211s) (loss=1.41725): 100%|| 352/352 [00:18<00:00, 18.99it/s]\n",
            "Validating student network Epoch [4] (40 / 40 Steps) (batch time=0.01688s) (loss=1.08645) (top1=75.00000) (top5=87.50000): 100%|| 40/40 [00:01<00:00, 34.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * acc@1 49.920 acc@5 95.520\n"
          ]
        }
      ],
      "source": [
        "distillation_box.compress_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "c20c3092c0956f53c057234954b0e6e2beb63c056f97db512353abbe859d97b3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
