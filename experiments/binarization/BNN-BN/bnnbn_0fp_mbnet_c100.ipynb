{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0861404",
   "metadata": {},
   "source": [
    "# BNN-BN\n",
    "This notebook demonstrates the implementation of the BNN-BN Paper\n",
    "# Steps to binarize the model\n",
    "- Load Dataset and DataLoader\n",
    "- Create the teacher model to be binarized\n",
    "- Load the configuration YAML file\n",
    "- create `BNNBN object` and pass the student model, dataloader, configuration\n",
    "- Binarize the model by using `compress_model` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba25adb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py117/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from trailmet.models import ModelsFactory\n",
    "from trailmet.datasets.classification import DatasetFactory\n",
    "from trailmet.algorithms.binarize.utils import Lighting\n",
    "from trailmet.algorithms.binarize.BNNBN import BNNBN\n",
    "import torchvision\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "957b031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31f7710d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DATASET': 'c100',\n",
       " 'arch': 'reactnet-A',\n",
       " 'binary_w': True,\n",
       " 'bn_type': 'none',\n",
       " 'batch_size': 256,\n",
       " 'workers': 4,\n",
       " 'valid_size': 0.1,\n",
       " 'num_train': 0,\n",
       " 'num_classes': 100,\n",
       " 'insize': 32,\n",
       " 'loss_type': 'ce',\n",
       " 'teacher': 'resnet34',\n",
       " 'teacher_weight': '',\n",
       " 'label_smooth': 0.1,\n",
       " 'pretrained': '',\n",
       " 'resume': False,\n",
       " 'save': './saved_weights',\n",
       " 'epoch': 5,\n",
       " 'agc': True,\n",
       " 'clip_value': 0.04,\n",
       " 'weight_decay': 0,\n",
       " 'learning_rate': 0.001,\n",
       " 'wandb': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = \"./\"\n",
    "import os\n",
    "\n",
    "with open(os.path.join(root_dir, \"bnnbn_cifar100.yaml\"), \"r\") as stream:\n",
    "    kwargs = yaml.safe_load(stream)\n",
    "kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd0e401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> SEEDING DONE\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(\"> SEEDING DONE\")\n",
    "\n",
    "\n",
    "set_seed(1024)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64ad2438",
   "metadata": {},
   "source": [
    "# Defining the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32aa3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Model = reactnet-A\n",
      "* Binarize both activation and weights\n",
      "* without BN\n"
     ]
    }
   ],
   "source": [
    "model = ModelsFactory.create_model(name=kwargs[\"arch\"], **kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19e8d708",
   "metadata": {},
   "source": [
    "# Augmentation, Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cf869e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Augmentation\n",
    "crop_scale = 0.08\n",
    "lighting_param = 0.1\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(32, scale=(crop_scale, 1.0)),\n",
    "        Lighting(lighting_param),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "valid_transform = transforms.Compose(\n",
    "    [transforms.Resize(36), transforms.CenterCrop(32), transforms.ToTensor()]\n",
    ")\n",
    "test_transform = valid_transform\n",
    "\n",
    "input_transforms = {\n",
    "    \"train\": train_transform,\n",
    "    \"val\": valid_transform,\n",
    "    \"test\": test_transform,\n",
    "}\n",
    "\n",
    "target_transforms = {\"train\": None, \"val\": None, \"test\": None}\n",
    "\n",
    "# Create Dataset\n",
    "cifar100_dataset = DatasetFactory.create_dataset(\n",
    "    name=\"CIFAR100\",\n",
    "    root=\"./\",\n",
    "    split_types=[\"train\", \"val\", \"test\"],\n",
    "    val_fraction=0.15,\n",
    "    transform=input_transforms,\n",
    "    target_transform=target_transforms,\n",
    ")\n",
    "# Define DataLoaders\n",
    "train_loader100 = torch.utils.data.DataLoader(\n",
    "    cifar100_dataset[\"train\"],\n",
    "    batch_size=kwargs[\"batch_size\"],\n",
    "    sampler=cifar100_dataset[\"train_sampler\"],\n",
    "    num_workers=kwargs[\"workers\"],\n",
    ")\n",
    "val_loader100 = torch.utils.data.DataLoader(\n",
    "    cifar100_dataset[\"val\"],\n",
    "    batch_size=kwargs[\"batch_size\"],\n",
    "    sampler=cifar100_dataset[\"val_sampler\"],\n",
    "    num_workers=kwargs[\"workers\"],\n",
    ")\n",
    "test_loader100 = torch.utils.data.DataLoader(\n",
    "    cifar100_dataset[\"test\"],\n",
    "    batch_size=kwargs[\"batch_size\"],\n",
    "    sampler=cifar100_dataset[\"test_sampler\"],\n",
    "    num_workers=kwargs[\"workers\"],\n",
    ")\n",
    "\n",
    "dataloaders = {\"train\": train_loader100, \"val\": val_loader100, \"test\": test_loader100}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b04a345d",
   "metadata": {},
   "source": [
    "# ReActNet Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f639059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manimesh-007\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/animesh_trailmet/experiments/binarization/BNN-BN/wandb/run-20230625_193820-duj3r0x7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/animesh-007/Trailmet%20BNNBN/runs/duj3r0x7' target=\"_blank\">CIFAR100_5_0.001_Jun-25_19:38:19</a></strong> to <a href='https://wandb.ai/animesh-007/Trailmet%20BNNBN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/animesh-007/Trailmet%20BNNBN' target=\"_blank\">https://wandb.ai/animesh-007/Trailmet%20BNNBN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/animesh-007/Trailmet%20BNNBN/runs/duj3r0x7' target=\"_blank\">https://wandb.ai/animesh-007/Trailmet%20BNNBN/runs/duj3r0x7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = BNNBN(model, dataloaders, **kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87899d6f",
   "metadata": {},
   "source": [
    "# Binarizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "758d68c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Model:reactnet(\n",
      "  (feature): ModuleList(\n",
      "    (0): firstconv3x3(\n",
      "      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (move11): LearnableBias()\n",
      "      (binary_3x3): HardBinaryConv()\n",
      "      (move12): LearnableBias()\n",
      "      (prelu1): PReLU(num_parameters=32)\n",
      "      (move13): LearnableBias()\n",
      "      (move21): LearnableBias()\n",
      "      (binary_pw_down1): HardBinaryConv()\n",
      "      (binary_pw_down2): HardBinaryConv()\n",
      "      (move22): LearnableBias()\n",
      "      (prelu2): PReLU(num_parameters=64)\n",
      "      (move23): LearnableBias()\n",
      "      (binary_activation): BinaryActivation()\n",
      "      (pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (move11): LearnableBias()\n",
      "      (binary_3x3): HardBinaryConv()\n",
      "      (move12): LearnableBias()\n",
      "      (prelu1): PReLU(num_parameters=64)\n",
      "      (move13): LearnableBias()\n",
      "      (move21): LearnableBias()\n",
      "      (binary_pw_down1): HardBinaryConv()\n",
      "      (binary_pw_down2): HardBinaryConv()\n",
      "      (move22): LearnableBias()\n",
      "      (prelu2): PReLU(num_parameters=128)\n",
      "      (move23): LearnableBias()\n",
      "      (binary_activation): BinaryActivation()\n",
      "      (pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (move11): LearnableBias()\n",
      "      (binary_3x3): HardBinaryConv()\n",
      "      (move12): LearnableBias()\n",
      "      (prelu1): PReLU(num_parameters=128)\n",
      "      (move13): LearnableBias()\n",
      "      (move21): LearnableBias()\n",
      "      (binary_pw): HardBinaryConv()\n",
      "      (move22): LearnableBias()\n",
      "      (prelu2): PReLU(num_parameters=128)\n",
      "      (move23): LearnableBias()\n",
      "      (binary_activation): BinaryActivation()\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (move11): LearnableBias()\n",
      "      (binary_3x3): HardBinaryConv()\n",
      "      (move12): LearnableBias()\n",
      "      (prelu1): PReLU(num_parameters=128)\n",
      "      (move13): LearnableBias()\n",
      "      (move21): LearnableBias()\n",
      "      (binary_pw_down1): HardBinaryConv()\n",
      "      (binary_pw_down2): HardBinaryConv()\n",
      "      (move22): LearnableBias()\n",
      "      (prelu2): PReLU(num_parameters=256)\n",
      "      (move23): LearnableBias()\n",
      "      (binary_activation): BinaryActivation()\n",
      "      (pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (move11): LearnableBias()\n",
      "      (binary_3x3): HardBinaryConv()\n",
      "      (move12): LearnableBias()\n",
      "      (prelu1): PReLU(num_parameters=256)\n",
      "      (move13): LearnableBias()\n",
      "      (move21): LearnableBias()\n",
      "      (binary_pw): HardBinaryConv()\n",
      "      (move22): LearnableBias()\n",
      "      (prelu2): PReLU(num_parameters=256)\n",
      "      (move23): LearnableBias()\n",
      "      (binary_activation): BinaryActivation()\n",
      "    )\n",
      "    (6): BasicBlock(\n",
      "      (move11): LearnableBias()\n",
      "      (binary_3x3): HardBinaryConv()\n",
      "      (move12): LearnableBias()\n",
      "      (prelu1): PReLU(num_parameters=256)\n",
      "      (move13): LearnableBias()\n",
      "      (move21): LearnableBias()\n",
      "      (binary_pw_down1): HardBinaryConv()\n",
      "      (binary_pw_down2): HardBinaryConv()\n",
      "      (move22): LearnableBias()\n",
      "      (prelu2): PReLU(num_parameters=512)\n",
      "      (move23): LearnableBias()\n",
      "      (binary_activation): BinaryActivation()\n",
      "      (pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (7): BasicBlock(\n",
      "      (move11): LearnableBias()\n",
      "      (binary_3x3): HardBinaryConv()\n",
      "      (move12): LearnableBias()\n",
      "      (prelu1): PReLU(num_parameters=512)\n",
      "      (move13): LearnableBias()\n",
      "      (move21): LearnableBias()\n",
      "      (binary_pw): HardBinaryConv()\n",
      "      (move22): LearnableBias()\n",
      "      (prelu2): PReLU(num_parameters=512)\n",
      "      (move23): LearnableBias()\n",
      "      (binary_activation): BinaryActivation()\n",
      "    )\n",
      "    (8): BasicBlock(\n",
      "      (move11): LearnableBias()\n",
      "      (binary_3x3): HardBinaryConv()\n",
      "      (move12): LearnableBias()\n",
      "      (prelu1): PReLU(num_parameters=512)\n",
      "      (move13): LearnableBias()\n",
      "      (move21): LearnableBias()\n",
      "      (binary_pw): HardBinaryConv()\n",
      "      (move22): LearnableBias()\n",
      "      (prelu2): PReLU(num_parameters=512)\n",
      "      (move23): LearnableBias()\n",
      "      (binary_activation): BinaryActivation()\n",
      "    )\n",
      "    (9): BasicBlock(\n",
      "      (move11): LearnableBias()\n",
      "      (binary_3x3): HardBinaryConv()\n",
      "      (move12): LearnableBias()\n",
      "      (prelu1): PReLU(num_parameters=512)\n",
      "      (move13): LearnableBias()\n",
      "      (move21): LearnableBias()\n",
      "      (binary_pw): HardBinaryConv()\n",
      "      (move22): LearnableBias()\n",
      "      (prelu2): PReLU(num_parameters=512)\n",
      "      (move23): LearnableBias()\n",
      "      (binary_activation): BinaryActivation()\n",
      "    )\n",
      "    (10): BasicBlock(\n",
      "      (move11): LearnableBias()\n",
      "      (binary_3x3): HardBinaryConv()\n",
      "      (move12): LearnableBias()\n",
      "      (prelu1): PReLU(num_parameters=512)\n",
      "      (move13): LearnableBias()\n",
      "      (move21): LearnableBias()\n",
      "      (binary_pw): HardBinaryConv()\n",
      "      (move22): LearnableBias()\n",
      "      (prelu2): PReLU(num_parameters=512)\n",
      "      (move23): LearnableBias()\n",
      "      (binary_activation): BinaryActivation()\n",
      "    )\n",
      "    (11): BasicBlock(\n",
      "      (move11): LearnableBias()\n",
      "      (binary_3x3): HardBinaryConv()\n",
      "      (move12): LearnableBias()\n",
      "      (prelu1): PReLU(num_parameters=512)\n",
      "      (move13): LearnableBias()\n",
      "      (move21): LearnableBias()\n",
      "      (binary_pw): HardBinaryConv()\n",
      "      (move22): LearnableBias()\n",
      "      (prelu2): PReLU(num_parameters=512)\n",
      "      (move23): LearnableBias()\n",
      "      (binary_activation): BinaryActivation()\n",
      "    )\n",
      "    (12): BasicBlock(\n",
      "      (move11): LearnableBias()\n",
      "      (binary_3x3): HardBinaryConv()\n",
      "      (move12): LearnableBias()\n",
      "      (prelu1): PReLU(num_parameters=512)\n",
      "      (move13): LearnableBias()\n",
      "      (move21): LearnableBias()\n",
      "      (binary_pw_down1): HardBinaryConv()\n",
      "      (binary_pw_down2): HardBinaryConv()\n",
      "      (move22): LearnableBias()\n",
      "      (prelu2): PReLU(num_parameters=1024)\n",
      "      (move23): LearnableBias()\n",
      "      (binary_activation): BinaryActivation()\n",
      "      (pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (13): BasicBlock(\n",
      "      (move11): LearnableBias()\n",
      "      (binary_3x3): HardBinaryConv()\n",
      "      (move12): LearnableBias()\n",
      "      (prelu1): PReLU(num_parameters=1024)\n",
      "      (move13): LearnableBias()\n",
      "      (move21): LearnableBias()\n",
      "      (binary_pw): HardBinaryConv()\n",
      "      (move22): LearnableBias()\n",
      "      (prelu2): PReLU(num_parameters=1024)\n",
      "      (move23): LearnableBias()\n",
      "      (binary_activation): BinaryActivation()\n",
      "    )\n",
      "  )\n",
      "  (pool1): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc): Linear(in_features=1024, out_features=100, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training in BNNBN Epoch [X] (X / X Steps) (batch time=X.Xs) (data time=X.Xs) (loss=X.X) (top1=X.X) (top5=X.X):   0%|| 0/167 [00:00<?, ?it/s]/opt/conda/envs/py117/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Training in BNNBN Epoch [0] (167 / 167 Steps) (batch time=0.20986s) (data time=0.00181s) (loss=4.11096) (top1=0.00000) (top5=25.00000): 100%|| 167/167 [00:14<00:00, 11.82it/s]\n",
      "Validating Epoch [0] (30 / 30 Steps) (batch time=0.16224s) (loss=4.35254) (top1=6.57895) (top5=17.10526): 100%|| 30/30 [00:01<00:00, 23.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * acc@1 2.760 acc@5 13.360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training in BNNBN Epoch [1] (167 / 167 Steps) (batch time=0.06216s) (data time=0.00189s) (loss=3.29017) (top1=50.00000) (top5=50.00000): 100%|| 167/167 [00:12<00:00, 13.78it/s]\n",
      "Validating Epoch [1] (30 / 30 Steps) (batch time=0.01842s) (loss=4.05580) (top1=7.89474) (top5=22.36842): 100%|| 30/30 [00:01<00:00, 26.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * acc@1 6.507 acc@5 23.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training in BNNBN Epoch [2] (167 / 167 Steps) (batch time=0.06210s) (data time=0.00218s) (loss=3.06706) (top1=0.00000) (top5=50.00000): 100%|| 167/167 [00:12<00:00, 13.72it/s] \n",
      "Validating Epoch [2] (30 / 30 Steps) (batch time=0.01805s) (loss=3.97032) (top1=11.84210) (top5=26.31579): 100%|| 30/30 [00:01<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * acc@1 11.240 acc@5 32.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training in BNNBN Epoch [3] (167 / 167 Steps) (batch time=0.06308s) (data time=0.00186s) (loss=3.39987) (top1=25.00000) (top5=25.00000): 100%|| 167/167 [00:11<00:00, 13.99it/s]\n",
      "Validating Epoch [3] (30 / 30 Steps) (batch time=0.01755s) (loss=3.55981) (top1=11.84210) (top5=40.78947): 100%|| 30/30 [00:01<00:00, 25.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * acc@1 14.827 acc@5 39.453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training in BNNBN Epoch [4] (167 / 167 Steps) (batch time=0.06289s) (data time=0.00193s) (loss=3.15846) (top1=0.00000) (top5=25.00000): 100%|| 167/167 [00:12<00:00, 13.61it/s] \n",
      "Validating Epoch [4] (30 / 30 Steps) (batch time=0.01780s) (loss=3.48275) (top1=17.10526) (top5=43.42105): 100%|| 30/30 [00:01<00:00, 25.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * acc@1 14.840 acc@5 39.453\n",
      "Total training time = 0.03377752906746335 hours\n",
      "Best acc = 14.84000015258789\n"
     ]
    }
   ],
   "source": [
    "fin = a.compress_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
