{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Growth Regularisation</h1>\n",
    "This notebook demonstrates the implementation of this paper <a href=https://arxiv.org/abs/2012.09243> Neural Pruning Via Growth Regularisation</a>\n",
    "<h4>Steps to train a baseline model and then compress it given a channel budget are as follows:</h4>\n",
    "<ul>\n",
    "    <li>Load the YAML file. </li>\n",
    "    <li>Load dataset and create dataloaders. </li>\n",
    "    <li>Create <b>Growth Regularisation</b> object and pass the parameters in the form of a dictionary. </li>\n",
    "    <li>Pass the dataloaders into the <b>compress_model</b> method to obtain the compressed model. </li>\n",
    "</ul>\n",
    "Since this is a demo notebook the number of epochs have been set to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py117/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import yaml\n",
    "\n",
    "from trailmet.datasets.classification import DatasetFactory\n",
    "from trailmet.models import ModelsFactory\n",
    "from trailmet.algorithms.prune.growth_regularisation import Growth_Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = \"./\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading the YAML file. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_classes': 100,\n",
       " 'weight_decay': 0.0005,\n",
       " 'arch': 'resnet50',\n",
       " 'dataset': 'cifar100',\n",
       " 'epochs': 1,\n",
       " 'learning_rate': 0.002,\n",
       " 'prune_ratio': 0.5,\n",
       " 'reg_upper_limit': 0.0004,\n",
       " 'reg_granularity_prune': 0.0001,\n",
       " 'method': 'GReg-1',\n",
       " 'wandb': True,\n",
       " 'base_pr_model': False,\n",
       " 'stage_pr': [0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
       " 'skip_layers': '',\n",
       " 'inherit_pruned': 'index',\n",
       " 'pick_pruned': 'min',\n",
       " 'copy_bn_w': True,\n",
       " 'copy_bn_b': True,\n",
       " 'wg': 'filter',\n",
       " 'print_interval': 100,\n",
       " 'lr_prune': 0.001,\n",
       " 'momentum': 0.9,\n",
       " 'lr_pick': 0.001,\n",
       " 'resume_path': False,\n",
       " 'update_reg_interval': 5,\n",
       " 'stabilize_reg_interval': 40000,\n",
       " 'mag_ratio_limit': 1000,\n",
       " 'reg_granularity_recover': 0.0001,\n",
       " 'plot_interval': 5,\n",
       " 'save_order_log': False,\n",
       " 'save_mag_reg_log': False,\n",
       " 'save_interval': 5,\n",
       " 'block_loss_grad': False,\n",
       " 'insize': 32}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(root, \"resnet50_cifar100.yaml\"), \"r\") as stream:\n",
    "    data_loaded = yaml.safe_load(stream)\n",
    "data_loaded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading CIFAR100Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Pad(4, padding_mode=\"reflect\"),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms1 = {\"train\": transform_train, \"val\": transform_test, \"test\": transform_test}\n",
    "\n",
    "target_transforms = {\"train\": None, \"val\": None, \"test\": None}\n",
    "\n",
    "cifar_dataset = DatasetFactory.create_dataset(\n",
    "    name=\"CIFAR100\",\n",
    "    root=\"./data\",\n",
    "    split_types=[\"train\", \"val\", \"test\"],\n",
    "    val_fraction=0.1,\n",
    "    transform=transforms1,\n",
    "    target_transform=target_transforms,\n",
    "    random_seed=42,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Creating the dataloaders</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(\n",
    "        cifar_dataset[\"train\"],\n",
    "        batch_size=64,\n",
    "        sampler=cifar_dataset[\"train_sampler\"],\n",
    "        num_workers=0,\n",
    "    ),\n",
    "    \"val\": torch.utils.data.DataLoader(\n",
    "        cifar_dataset[\"val\"],\n",
    "        batch_size=64,\n",
    "        sampler=cifar_dataset[\"val_sampler\"],\n",
    "        num_workers=0,\n",
    "    ),\n",
    "    \"test\": torch.utils.data.DataLoader(\n",
    "        cifar_dataset[\"test\"],\n",
    "        batch_size=64,\n",
    "        sampler=cifar_dataset[\"test_sampler\"],\n",
    "        num_workers=0,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating the method's object proceed with compression. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelsFactory.create_model(name=\"resnet50\", pretrained=False, **data_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manimesh-007\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/animesh_trailmet/experiments/pruning/growth_regularization/wandb/run-20230627_215158-nx7komlz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/animesh-007/Trailmet%20Growth_Regularisation/runs/nx7komlz' target=\"_blank\">CIFAR100_120_0.002_Jun-27_21:51:56</a></strong> to <a href='https://wandb.ai/animesh-007/Trailmet%20Growth_Regularisation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/animesh-007/Trailmet%20Growth_Regularisation' target=\"_blank\">https://wandb.ai/animesh-007/Trailmet%20Growth_Regularisation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/animesh-007/Trailmet%20Growth_Regularisation/runs/nx7komlz' target=\"_blank\">https://wandb.ai/animesh-007/Trailmet%20Growth_Regularisation/runs/nx7komlz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "slim = Growth_Regularisation(model, dataloaders, **data_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim.compress_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f946df053fbf2b937619d3c5458e7af74262f9a954d8797ba0b27400bcafe06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
